# sonmari
수어 번역 프로그램입니다.

# 프로그램 목적
청각장애인과 수어를 모르는 비장애인은 서로 소통하기가 어렵다. 특히 병원에서 청각장애인이 제대로 진료받지 못하는 사례가 늘고 있는데,
이를 위해 병원에서 쓰는 수어를 번역할 수 있는 어플을 만들기로 했다.
사용자가 카메라렌즈를 통해 직접 수어영상을 촬영하면, 이를 한국어로 번역해주는 것이다.

수어영상을 입력받아 손모양을 인식하고, 이를 번역하여 음성언어로 바꿔준 뒤 출력해준다.
![틀](https://user-images.githubusercontent.com/74365895/101729534-a71f9800-3afb-11eb-8a29-49b4a98d48f5.jpg)

# 필요 기술

정확한 인식을 위해 두 가지 방법을 사용했다. 먼저 이미지의 외곽선인 컨투어를 검출하여 컨투어에 대한 convexhull을 계산한다. 초록색 선이 Convex hull이며 손끝의 검은 원이 손가락 후보이다. 선의 방향이 바뀌는 지점을 손가락 후보로 한다. 
다음으로는 컨투어를 근사하고 디펙트를 구하여 손가락을 검출한다. 두 개의 손가락 사이마다 위치한 디펙트를 이용하여 뾰족한 부분을 손가락 후보로 한다.
Convexhull을 사용하여 검출한 손가락 후보와 디펙트를 사용하여 검출한 손가락 후보를 합쳐서 사용합니다. 손가락 후보가 발견된 위치는 두 엣지가 만나는 곳이다. 이 점을 이용하여 좌우에 있는 엣지가 이루는 각도가 90도 이하인 경우에만 손가락으로 인식하도록 했다. 그 결과 손가락 후보 중 잘못 인식된 것들이 제거된다.

이렇게 인식한 수어영상을 토대로 cnn과 rnn을 이용해 한국어로 번역하기로 했다. 입력받은 영상을 프레임 단위로 여러개의 영상으로 자른다. 이후 각각의 작은 영상을 cnn을 이용해 특성을 추출해내고, 이 여러개의 시퀀스데이터를 다시 rnn에 입력하여 전체적인 번역결과를 도출하는 것이 번역 파트의 핵심이다.

아직 초기 단계라 영상 번역까지는 진행하지 못했다. 
우선 이미지부터 번역해보고 이후에 영상을 번역하기로 했다. 숫자 1,2,3,4,5를 번역하는 것이다. 
각 숫자별로 50*50 픽셀 크기인 2400장의 학습용 이미지를 수집했고, 같은 숫자를 의미하는 데이터들은 모두 하나의 폴더에 저장해두었다. 이후 데이터베이스를 만들어 폴더명과 숫자를 매칭시키고
이를 토대로 training, test 시켰다.
사용한 신경망의 구조는 다음과 같다.

<img width="334" alt="신경망" src="https://user-images.githubusercontent.com/74365895/101729589-c61e2a00-3afb-11eb-9653-f9ff3c19d05c.png">
이미지 데이터를 덴스 레이어로 번역하는 과정인데, 컨볼루션 레이어와 풀링 레이어를 반복하여 각각 총 세겹으로 구성하고, 플래튼,덴스, 드롭아웃 레이어를 통해 과적합을 막는다.
이후 케라스를 이용해 예측의 정확도와 예측한 결과값을 반환받은 뒤, 정확도가 80프로 이상일 경우에만 결과값을 출력한다.


# reference
github.com/EvilPort2/Sign-Language

# license
해당사항 없음


